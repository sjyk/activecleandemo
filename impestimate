\section{Approximating the Optimal Distribution}
We do not know what the clean value of the gradient is so we have to estimate this clean value.
We want to derive our estimate in such a way that we can decouple the different cleaners.
For each cleaner $i$, we should be able to estimate a $\Delta_i$ which is an expected incremental effect of the cleaner.
In this section, we formalize this problem with a first order approximation of the expected gradient.

The expected gradient is defined as:
\[
\mathbb{E}(\nabla\phi(x_{clean},y_{clean},\theta^{(t)}))
\]
Since, we do not know this value, we can take the expected value of the Taylor series expansion around the dirty value.
Let us first consider the case where a single column is updated. 
There are two cases: (1) either a feature $X$ is updated or an obersvation $Y$ is updated.

\noindent \textbf{Feature Cleaned: } 
\[
\nabla\phi(x,y,\theta^{(t)}) + \frac{\partial}{\partial X}\nabla\phi(x,y,\theta^{(t)})\cdot \mathbb{E}(\Delta x)+...~~\blacksquare
\]

\noindent \textbf{Observation Cleaned: } 
\[
\nabla\phi(x,y,\theta^{(t)}) + \frac{\partial}{\partial Y}\nabla\phi(x,y,\theta^{(t)})\cdot \mathbb{E}(\Delta y)+...~~\blacksquare
\]

The linearity of the Taylor series expansion allows us to sum up the $\Delta$ changes for each column.
\begin{lemma}[Additive Changes]
For changes to all columns, the Taylor Series expansion is:
\[\nabla\phi(x,y,\theta^{(t)}) + \sum_i \frac{\partial}{\partial X_i}\nabla\phi(x,y,\theta^{(t)})\cdot \mathbb{E}(\Delta_i x) \]
\[+ \sum_j \frac{\partial}{\partial Y_j}\nabla\phi(x,y,\theta^{(t)})\cdot \mathbb{E}(\Delta_j y)\]
\end{lemma}
\noindent Since cleaners operate on disjoint attributes:
\begin{theorem}[Cleaner Decoupling]
For a set of cleaners $C_{1} \circ C_{2} \circ ... C_{k}$, we can associate a vector $\bar{\Delta}_i$ which is equal to:
\[
\bar{\Delta}_i = \sum_i \frac{\partial}{\partial X_i}\nabla\phi(x,y,\theta^{(t)})\cdot \mathbb{E}(\Delta_i x)
\]
\end{theorem}

\subsection{Approximation Quality}
Next, we explore the quality of this approximation.
The error in the Taylor series is given by the higher derivatives:
\[
+ \frac{1}{2}\frac{\partial^2}{\partial X_i}\nabla\phi(x,y,\theta^{(t)}\cdot \mathbb{E}(\Delta_i x^2) + \frac{1}{6}\frac{\partial^3}{\partial X_i}\nabla\phi(x,y,\theta^{(t)}\cdot \mathbb{E}(\Delta_i x^3) + ...
\]
For many common loss functions, these higher derivatives are zero.


\subsection{Relative Benefit}

We analyze the conditions under which this estimate results in a more accurate 




\subsection{Putting it into practice}
Part of the motivation of this work is that the function $\mathcal{D}$ is often expensive to evaluate.
Instead, suppose we had an estimate of $\hat{\mathcal{D}}$ that was cheap to evaluate.
We could then apply this estimate to every point and get an approximately optimal sampling distribution.
This builds the inutition for the basic algorithm that we will propose.
As we clean data, we learn an estimator $\hat{C}$ which can be used to calculate the approximately optimal sampling distribution.


