\section{Estimation}\label{sampling}
This section makes the sampling result of the previous section practical by approximating the cleaned values.

\subsection{Challenges and Goal}
The optimal sampling distribution is dependent on a value that is not known without data cleaning $\nabla\phi(x^{(c)}_i,y^{(c)}_i,\theta^{(t)})$.
One way to approximate this distribution is to learn a function $e(\cdot)$ via regression based on previously cleaned data.
This is a high-dimensional regression problem which may have to learn a very complicated relationship between dirty and clean data.
The biggest challenge with such an estimator is the cold start problem, where if given a small amount of cleaned data, the estimator will be inaccurate.
\sys should be optimized to make as much progress as possible in the early iterations so this technique may not work.
The estimator takes an alternative approach where it exploits information from the detector to produce an estimate for groups of similarly corrupted records.

\subsection{Estimation For A Priori Detection}
\begin{example}
Suppose records from running example dataset are corrupted with both entity resolution problems, missing data, and constraint violations. 
Each training example will have a set of corrupted features (e.g., $\{1,2,6\}$, $\{1,2,15\}$).

Suppose that the cleaner has just cleaned the records $r_1$ and $r_2$ represented as tuples with their corrupted feature set: ($r_1$,$\{1,2,3\}$), ($r_2$,$\{1,2,6\}$).
Then, given a new record ($r_3$,$\{1,2,3,6\}$). 
The estimator should be able to use the cleaning results from $r_1,r_2$ to estimate the gradient in $r_3$.
\end{example}

If most of the features are correct, it would seem like the gradient is only
incorrect in one or two of its components.
The problem is that the gradient $\nabla\phi(\cdot)$ can be a very non-linear function of the features that couple features together.
For example, the gradient for linear regression is:
\[
\nabla\phi(x,y,\theta) = (\theta^Tx - y)x
\]
It is not possible to isolate the effect of a change of one feature on the gradient.
Even if one of the features is corrupted, all of the gradient components will be incorrect.

\subsubsection{Error Decoupling}
To address this problem, the gradient can be approximated in a way that the effects of dirty features on the gradient are decoupled.
Recall, in the \emph{a priori} detection problem, that associated with each $r \in R_{dirty}$ is a set of errors $f_r,l_r$ which is a set that identifies a set of corrupted features and labels.
This property can be used to construct a coarse estimate of the clean value.
The main idea is to calculate average changes for each feature, then given an uncleaned (but dirty) record, add these average changes to correct the gradient.

To formalize this intuition, instead of computing the actual gradient with respect to the 
true clean values, compute the conditional expectation given that a set of features and labels $f_r,l_r$ are corrupted:
\[
p_i \propto \mathbb{E}(\nabla\phi(x^{(c)}_i,y^{(c)}_i,\theta^{(t)}) \mid f_r,l_r)
\]
Corrupted features defined as that:
\[
i \notin f_r \implies x^{(c)}[i] - x^{(d)}[i] = 0
\]
\[
i \notin l_r \implies y^{(c)}[i] - y^{(d)}[i] = 0
\]

The needed approximation represents a linearization of the errors, and the resulting approximation will be of the form:
\[
p_{r}\propto\|\nabla\phi(x,y,\theta^{(t)}) + M_x \cdot \Delta_{rx} +  M_y \cdot \Delta_{ry}\|
\]
where $M_x$, $M_y$ are matrices and $\Delta_{rx}$ and $\Delta_{ry}$ are average change vectors for the corrupted features in $r$. 
Without this approximation, calculating the expected value conditioned on $f_r,l_r$ would require conditioning on all the combinatorial possibilities.

\subsubsection{Deriving $M_x$, $M_y$}
If $d$ is the dirty value and $c$ is the clean value, the Taylor series approximation for a function $f$ is given as follows:
\[
f(c) = f(d) + f'(d)\cdot(d-c) + ...
\]
Ignoring the higher order terms, the linear term $f'(d)\cdot(d-c)$ is a linear function in each feature and label.
Then, taking expected values, it follows that:
\[
\approx \nabla\phi(x,y,\theta) + M_x \cdot \mathbb{E}(\Delta x) + M_y \cdot \mathbb{E}(\Delta y)
\]
where $M_x = \frac{\partial}{\partial X}\nabla\phi$ and $M_y = \frac{\partial}{\partial Y}\nabla\phi$ (See Appendix \ref{taylor-deriv} for derivation).
Recall that the feature space is $d$ dimensional and label space is $l$ dimensional.
Then, $M_x$ is an $d \times d$ matrix, and $M_y$ is a $d \times l$ matrix.
Both of these matrices are computed for each record (see Appendix \ref{example-deriv} for an example derivation).
$\Delta x$ is a $d$ dimensional vector where each component represents a change in that feature and $\Delta y$ is an $l$ dimensional vector that represents the change in each of the labels. 

\subsubsection{More Accurate Early Error Estimates}\label{acc}
Linearization over avoids amplifying estimation error.
Consider the linear regression gradient:
\[
\nabla\phi(x,y,\theta) = (\theta^Tx - y)x
\]
This can be rewriten as a vector in each component:
\[
g[i] = \sum_{i} x[i]^2-x[i]y + \sum_{j \ne i} \theta[j]x[j]
\]
This function is already mostly linear in $x$ except for the one quadratic term.
However, this one quadratic term has potential to amplify errors.
Consider two expressions:
\[
f(x+\epsilon) = (x+\epsilon)^2 = x^2 + 2x\epsilon + \epsilon^2
\]
\[
f(x+\epsilon) \approx f(x) + f'(x)(\epsilon) = x^2 + 2x\epsilon
\]
The only difference between the two estimates is the quadratic $\epsilon^2$, if $\epsilon$ is highly uncertain random variable then the quadratic dominates.
If this variance is large, the Taylor estimate avoids amplifying this error.
Of course, this is at the tradeoff of some additional bias since the true function is non-linear.
We evaluate this linearization in Section \ref{est} against alternatives, and find that indeed it provides more accurate estimates for a small number of samples cleaned.
When the number of cleaned samples is large the alternative techniques are comparable or even slightly better.


\subsubsection{Maintaining Decoupled Averages}
This linearization allows \sys to maintain per feature (or label) average changes and use these changes to center the optimal sampling distribution around the expected clean value.
To estimate $\mathbb{E}(\Delta x)$ and $\mathbb{E}(\Delta y)$, consider the following lemma:
\begin{lemma}[Single Feature]
For a feature $i$, we average all $j=\{1,...,K\}$ records cleaned that have an error for that feature, weighted by their sampling probability:
\[
\bar{\Delta}_{xi} = \frac{1}{NK}\sum_{j=1}^K (x^{(d)}[i]-x^{(c)}[i])\times \frac{1}{p(j)}
\]
Similarly, for a label $i$:
\[
\bar{\Delta}_{yi} = \frac{1}{NK}\sum_{j=1}^K (y^{(d)}[i]-y^{(c)}[i])\times \frac{1}{p(j)}
\]
\end{lemma}

Each $\bar{\Delta}_{xi}$ and $\bar{\Delta}_{yi}$ represents an average change in a single feature.
A single vector can represent the necessary changes to apply to a record $r$:
\begin{lemma}[Delta vector]
For a record $r$, the set of corrupted features is $f_r,l_r$.
Then, each record $r$ has a d-dimensional vector $\Delta_{rx}$ which is constructed as follows:
\[
 \Delta_{rx}[i] = \begin{cases} 0 & i \notin f_r \\ 
\bar{\Delta}_{xi} & i \in f_r
\end{cases} 
\]
Each record $r$ also has an l-dimensional vector $\Delta_{ry}$ which is constructed as follows:
\[
 \Delta_{rx}[i] = \begin{cases} 0 & i \notin l_r \\ 
\bar{\Delta}_{yi} & i \in l_r
\end{cases} 
\]
\end{lemma}

Finally, the result is: 
\[p_{r}\propto\|\nabla\phi(x,y,\theta^{(t)}) + M_x \cdot \Delta_{rx} +  M_y \cdot \Delta_{ry}\|
\blacksquare
\]

\subsection{Estimation For Adaptive Case}
A similar procedure holds in the adaptive setting, however, it requires reformulation of ``similarly corrupted".
Here, \sys uses $u$ corruption classes provided by the detector.
Instead of conditioning on the features that are corrupted, the estimator conditions on the classes.
So for each error class, it computes a $\Delta_{ux}$ and $\Delta_{uy}$.
These are the average change in the features given that class and the average change in labels given that class.
\[
p_{r,u}\propto\|\nabla\phi(x,y,\theta^{(t)}) + M_x \cdot \Delta_{ux} +  M_y \cdot \Delta_{uy}\|
\blacksquare
\] 

