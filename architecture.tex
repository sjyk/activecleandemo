\section{Architecture and Overview}\label{arch}
We will first describe \sys and overview the entire framework, and a detailed description of the research challenges and algorithms can be found in~\cite{activecleanarxiv}.

\subsection{What Is New?}
Machine learning, specifically active learning, has been applied in prior work to improve the efficiency of data cleaning~\cite{yakout2013don,DBLP:journals/pvldb/YakoutENOI11,gokhale2014corleone}.
Human input, either for cleaning or validation of automated cleaning, is often expensive and impractical for large datasets.
A model can learn rules from a small set of examples cleaned (or validated) by a human, and active learning is a technique to carefully select the set of examples to learn the most accurate model.
This model can be used to extrapolate repairs to not-yet-cleaned data, and the goal of these approaches is to provide the cleanest possible dataset--independent of the subsequent analytics or query processing.

Prior work studies applying machine learning to facilitate improved data cleaning, and in contrast, \sys explores the problem of the impact of data cleaning on user-specified models.
This new problem setting leads to questions of correctness and ``push-down" prioritization, i.e., how information about a user's subsequent data analysis (the model that is trained) can inform which data to clean.
Existing approaches, which are designed for homogeneous data, cannot be applied here.
One of the primary contributions of this work is an incremental model update algorithm with correctness guarantees for mixtures of data.
The essence of the approach is to model the human's iterative process as a Stochastic Gradient Descent loop.

\subsection{Problem Setup and Formalization}
There is a relation $R$ and we wish to train a model using the data in $R$.
We assume that there is a featurizer $F(\cdot)$ that maps every record $r \in R$ to a feature vector $x$ and label $y$.
This work focuses on a class of well-analyzed predictive analytics problems; ones that can be expressed as the minimization of loss functions.
For labeled training examples $\{(x_{i},y_{i})\}_{i=1}^{N}$, the problem is to find a vector of \emph{model parameters} $\theta$ by minimizing a loss function $\phi$ over all training examples:
\[
 \theta^{*}=\arg\min_{\theta}\sum_{i=1}^{N}\phi(x_{i},y_{i},\theta)
\]
Where $\phi$ is a convex function in $\theta$.
For example, in a linear regression $\phi$ is:
\[
\phi(x_{i},y_{i},\theta) = \|\theta^Tx_{i} - y_i \|_2^2
\]
Typically, a \emph{regularization} term $r(\theta)$ is added to this problem.
$r(\theta)$ penalizes high or low values of feature weights in $\theta$ to avoid overfitting to noise in the training examples.
\begin{equation}
 \theta^{*}=\arg\min_{\theta}\sum_{i=1}^{N}\phi(x_{i},y_{i},\theta) + r(\theta)
 \label{ideal}
\end{equation}
In this work, without loss of generality, we will include the regularization as part of the loss function i.e., $\phi(x_{i},y_{i},\theta)$ includes $r(\theta)$.

\subsection{Required User Input}\label{uinp}
More concretely, the analyst provides the following user-defined function to use \sys:

\noindent\textbf{Model:} The user provides a predictive model (e.g., SVM) specified as a loss optimization problem $\phi(\cdot)$ and a featurizer $F(\cdot)$ that maps a record to its feature vector $x$ and label $y$.

\vspace{0.25em}

\noindent\textbf{Gradient:} The user provides a gradient function $\nabla\phi(\cdot)$ that returns the gradient of the loss. For popular convex models such as SVMs and Linear Regression these functions are known and easy to express analytically. For more complex problems such Topic Modeling or Neural Network learning, we assume that this function (or an approximation of it) is expressed programatically. There are a number of frameworks such as Torch, Theano, and TensorFlow, which can return such programs using symbolic differentiation. 

\vspace{0.25em}

\noindent\textbf{Batches: } Data are cleaned in batches of size $b$ and the user can change these settings if she desires more or less frequent model updates.
We empirically find that a batch size of $50$ performs well across different datasets and use that as a default.
A cleaning budget $k$ can be used as a stopping criterion once $C(\dot)$ has been called $k$ times, and so the number of iterations of \sys is $T = \frac{k}{b}$.
Alternatively, the user can clean data until the model is of sufficient accuracy to make a decision.

\vspace{0.25em}

\noindent\textbf{Cleaning Function: } We represent this operation as $Clean(\cdot)$ which can be applied to a record $r$ (or a set of records) to recover the clean record $r' = Clean(r)$.
Formally, we treat the $Clean(\cdot)$ as an expensive user-defined function (implemented by code or by manual inspection) composed of deterministic schema-preserving \textsf{map} and \textsf{filter} operations applied to a subset of rows in the relation. 

\subsection{Basic Data Flow} \label{df}
The system first trains the model $\phi(\cdot)$ on the dirty dataset to find an initial model $\theta^{(d)}$ that the system will subsequently improve.
The {\it sampler} selects a sample of size $b$ records from the dataset and passes
the sample to the {\it cleaner}, which executes $Clean(\cdot)$ for each sample record and outputs their cleaned versions.
The \emph{updater} uses the cleaned sample to update the weights of the model, thus moving the model closer to the true cleaned model (in expectation).
Finally, the system either terminates due to a stopping condition (e.g., $C(\cdot)$ has been called a maximum number of times $k$, or training error convergence),
or passes control to the {\it sampler} for the next iteration.

  \noindent To summarize in pseudocode:
  \begin{enumerate}[leftmargin=1em]\scriptsize\sloppy
  \item \texttt{Init(dirty\_data, cleaned\_data, dirty\_model, batch, iter)}
  \item For each t in $\{1,...,T\}$
  \begin{enumerate}
    \item \texttt{dirty\_sample $=$ sampler(dirty\_data, sample\_prob, detector, batch)}
    \item \texttt{clean\_sample $=$ Cleaner(dirty\_sample)}
    \item \texttt{current\_model $=$ Update(current\_model, sample\_prob, clean\_sample)}
    \item \texttt{cleaned\_data = cleaned\_data + clean\_sample}
    \item \texttt{dirty\_data = dirty\_data - clean\_sample}
    \item \texttt{sample\_prob $=$ Estimator(dirty\_data, cleaned\_data, detector)}
    \item \texttt{detector $=$ DetectorUpdater(detector, cleaned\_data)}
  \end{enumerate}
  \item \texttt{Output: current\_model}
  \end{enumerate}

\subsection{Optimizations}
We overview some of the research contributions and the details can be found in~\cite{activecleanarxiv}.

  \vspace{0.5em}

  \noindent\textbf{Gradient-Based Updates: } Rather than retraining, in \sys, we start with a dirty model as an initialization, and then incrementally make an update using a gradient step.We can draw an analogy to Materialized View maintenance, since after all, a model parametrized by $\theta$ is just a table of floating point numbers.
  This process leverages the structure of the model rather than treating it like a black-box, and we apply convergence arguments from optimization theory.

  \vspace{0.5em}

  \noindent\textbf{Estimate-Driven Prioritization: } We use an importance sampling technique to select a sample of likely dirty records. This is designed in a way so it still preserves convergence guarantees.

  \vspace{0.5em}

  \noindent\textbf{Partitioning Dirty and Clean Data: } \sys adaptively learns to partition dirty and clean data based on the analysts actions.
  Partitioning serves two purposes: (1) it reduces the variance of our updates because we can cheaply scan over data we know that is clean, and (2) it increases the fraction of actually dirty records in the candidate batch.
