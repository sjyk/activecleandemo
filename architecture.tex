\section{Architecture and Overview}\label{arch}
We will first describe \sys and overview the entire framework, and a detailed description of the research challenges and algorithms can be found in~\cite{activecleanarxiv}.

\subsection{What Is New?}
Machine learning, specifically active learning, has been applied in prior work to improve the efficiency of data cleaning~\cite{yakout2013don,DBLP:journals/pvldb/YakoutENOI11,gokhale2014corleone}.
Human input, either for cleaning or validation of automated cleaning, is often expensive and impractical for large datasets.
A model can learn rules from a small set of examples cleaned (or validated) by a human, and active learning is a technique to carefully select the set of examples to learn the most accurate model.
This model can be used to extrapolate repairs to not-yet-cleaned data, and the goal of these approaches is to provide the cleanest possible dataset--independent of the subsequent analytics or query processing.

To summarize, prior work studies how to \emph{use machine learning models} to improve data cleaning.
In contrast, \sys explores how to control the impact of data cleaning for downstream machine learning models.
This new problem setting leads a question of correctness -- if I incrementally clean subsets of my data, is the model I then train even correct?  
It also leads to optimization opportunities -- how can the data cleaning step be made aware of the subsequent data analysis (e.g., the model that is trained) 
in such a way that the model will most quickly converge to the correct model?
Existing approaches, which are designed for homogeneous (all clean or all dirty) data, cannot be applied here.
One of the primary contributions of this work is an incremental model update algorithm with correctness guarantees for the resulting mixture of dirty and clean data.
The essence of the approach is to model the human's iterative process as a Stochastic Gradient Descent loop.

\subsection{Problem Setup and Formalization}
There is a relation $R$ and we wish to train a model using the data in $R$.
We assume that there is a featurizer $F(\cdot)$ that maps every record $r \in R$ to a feature vector $x$ and label $y$.
This work focuses on a class of well-analyzed predictive analytics problems, ones that can be expressed as the minimization of loss functions, that will be trained on the output of applying $F(\cdot)$ to the records in $R$.
For these labeled training examples $\{(x_{i},y_{i})\}_{i=1}^{N}$, the problem is to find a vector of \emph{model parameters} $\theta$ by minimizing a loss function $\phi$ over all training examples:
\[
 \theta^{*}=\arg\min_{\theta}\sum_{i=1}^{N}\phi(x_{i},y_{i},\theta)
\]
Where $\phi$ is a convex function in $\theta$.
For example, in a linear regression $\phi$ is:
\[
\phi(x_{i},y_{i},\theta) = \|\theta^Tx_{i} - y_i \|_2^2
\]
Typically, a \emph{regularization} term $r(\theta)$ is added to this problem.
$r(\theta)$ penalizes high or low values of feature weights in $\theta$ to avoid overfitting to noise in the training examples.
\begin{equation}
 \theta^{*}=\arg\min_{\theta}\sum_{i=1}^{N}\phi(x_{i},y_{i},\theta) + r(\theta)
 \label{ideal}
\end{equation}
In this work, without loss of generality, we will include the regularization as part of the loss function i.e., $\phi(x_{i},y_{i},\theta)$ includes $r(\theta)$.
We note that designing \sys for this form of machine learning model supports SVMs, Linear Regression, Logistic Regression, Neural Networks, Latent Dirichlet Allocation, and Gaussian Mixture Models.

\subsection{Required User Input}\label{uinp}
In such a problem setting, \sys takes as input four user-defined parameters.  The first two are UDFs used in order to train models, and can be assumed to be readily available.  
The latter two consist of a user tunable parameters with reasonable defaults,
and a generic record cleaning function implemented by script or by manual effort:

\noindent\textbf{Model:} The user provides a predictive model (e.g., SVM) specified as a loss optimization problem $\phi(\cdot)$ and a featurizer $F(\cdot)$ that maps a record to its feature vector $x$ and label $y$.

\vspace{0.25em}

\noindent\textbf{Gradient:} The gradient function $\nabla\phi(\cdot)$ returns the gradient of the loss. 
For popular convex models such as SVMs and Linear Regression these functions are known and provided as part of the system. 
For more complex problems such Topic Modeling or Neural Network learning, we assume that this function (or an approximation of it) is expressed programatically. There are a number of frameworks such as Torch, Theano, and TensorFlow, which can return such programs using symbolic differentiation. 

\vspace{0.25em}

\noindent\textbf{Stopping Criteria: } Data are cleaned in batches of size $b$ and the user can change these settings if she desires more or less frequent model updates.
We empirically find that a batch size of $50$ performs well across different datasets and use that as a default.
A cleaning budget $k$ can be used as a stopping criterion once $C(\dot)$ has been called $k$ times, and so the number of iterations of \sys is $T = \frac{k}{b}$.
Alternatively, the user can clean data until the model is of sufficient accuracy to make a decision.

\vspace{0.25em}

\noindent\textbf{Cleaning Function: } We represent this operation as $Clean(\cdot)$ which can be applied to a record $r$ (or a set of records) to recover the clean record $r' = Clean(r)$.
Formally, we treat the $Clean(\cdot)$ as an expensive user-defined function (implemented as code or  manual inspection) composed of deterministic schema-preserving \textsf{map} and \textsf{filter} operations applied to a subset of rows in the relation. 
This formulation supports common operations such as extraction, deletion, transformation, and find-and-replace.

\subsection{Basic Data Flow} \label{df}
The following pseudocode summarizes how \sys works:

  \begin{enumerate}[leftmargin=1em]\scriptsize\sloppy
  \item \texttt{Init(dirty\_data, cleaned\_data, dirty\_model, batch, iter)}
  \item For each t in $\{1,...,T\}$
  \begin{enumerate}
    \item \texttt{dirty\_sample $=$ Sampler(dirty\_data, sample\_prob, detector, batch)}
    \item \texttt{clean\_sample $=$ Cleaner(dirty\_sample)}
    \item \texttt{current\_model $=$ Updater(current\_model, sample\_prob, clean\_sample)}
    \item \texttt{cleaned\_data = cleaned\_data + clean\_sample}
    \item \texttt{dirty\_data = dirty\_data - clean\_sample}
    \item \texttt{sample\_prob $=$ Estimator(dirty\_data, cleaned\_data, detector)}
    \item \texttt{detector $=$ Detector(detector, cleaned\_data)}
  \end{enumerate}
  \item \texttt{Output: current\_model}
  \end{enumerate}

The system first trains the model $\phi(\cdot)$ on the dirty dataset to find an initial model $\theta^{(d)}$ that the system will subsequently improve.
The {\it Sampler} selects a sample of size $b$ records from the dataset and passes
the sample to the {\it Cleaner}, which executes $C(\cdot)$ on the whole sample and outputs their cleaned versions.
The \emph{Updater} uses the cleaned sample to update the weights of the model, thus moving the model closer to the true cleaned model (in expectation).
Finally, the system either terminates due to a stopping condition (e.g., $C(\cdot)$ has been called a maximum number of times $k$, or training error convergence),
or passes control to the {\it sampler} for the next iteration.
\sys assumes that {\it Init} and {\it Cleaner} are user-specified, and it implements all of the other functions.

\subsection{Technical Details}
We overview some of the research contributions and the details can be found in~\cite{activecleanarxiv}.

  \vspace{0.5em}

  \noindent\textbf{Updater: } Rather than retraining, in \sys, we start with a dirty model as an initialization, and then incrementally make an update using a gradient step.
  This process leverages the structure of the model rather than treating it like a black-box, and we apply convergence arguments from optimization theory.

  \vspace{0.5em}

  \noindent\textbf{Estimator and Sampler: } We use an importance sampling technique to select a sample of likely dirty records. This is designed in a way so it still preserves convergence guarantees.

  \vspace{0.5em}

  \noindent\textbf{Detector: } \sys adaptively learns to partition dirty and clean data based on the analysts actions.
  Partitioning serves two purposes: (1) it reduces the variance of our updates because we can cheaply scan over data we know that is clean, and (2) it increases the fraction of actually dirty records in the candidate batch.
